{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéß AUDIO INGEST + VAD SPLIT (Silero) + TRANSCRIBE + CHUNK + SUMMARIZE\n",
    "\n",
    "from pathlib import Path\n",
    "import json, math, subprocess, sys\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# === CONFIG ===\n",
    "AUDIO_ROOT = Path(\"/mnt/d/Roshidat_Msc_Project/AI_Project/Company dataset/Mytrainingdataset/AllAudio\")\n",
    "AUDIO_OUT  = Path(\"./audio_outputs\"); AUDIO_OUT.mkdir(parents=True, exist_ok=True)\n",
    "AUDIO_EXTS = {\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\", \".wma\", \".aac\", \".opus\"}\n",
    "\n",
    "\n",
    "CHUNK_SETTINGS = dict(max_characters=10_000, new_after_n_chars=6_000, combine_text_under_n_chars=2_000)\n",
    "\n",
    "\n",
    "FW_MODEL_NAME   = \"small\"      \n",
    "FW_COMPUTE_TYPE = \"int8\"       \n",
    "\n",
    "\n",
    "FIXED_SEGMENT_SECONDS = 600\n",
    "\n",
    "\n",
    "USE_VAD = True                \n",
    "VAD_MIN_SPEECH = 0.6          \n",
    "VAD_MAX_SPEECH = 300.0         \n",
    "VAD_PAD = 0.15                \n",
    "VAD_MERGE_GAP = 0.35          \n",
    "\n",
    "\n",
    "audio_file_stats: Dict[str, Dict] = {}\n",
    "all_audio_segments: List[Dict] = []\n",
    "all_audio_chunks: List[Dict] = []\n",
    "all_audio_transcripts: List[Dict] = []\n",
    "\n",
    "\n",
    "def _pip(*args):\n",
    "    print(\">\", sys.executable, \"-m\", \"pip\", *args)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", *args])\n",
    "\n",
    "def _ensure_pkgs():\n",
    "    try:\n",
    "        import faster_whisper  \n",
    "    except ImportError:\n",
    "        _pip(\"install\", \"faster-whisper==1.0.3\")\n",
    "    try:\n",
    "        from pydub import AudioSegment  \n",
    "    except ImportError:\n",
    "        _pip(\"install\", \"pydub\")\n",
    "    if USE_VAD:\n",
    "        try:\n",
    "            import torch  \n",
    "        except ImportError:\n",
    "            _pip(\"install\", \"torch\")  \n",
    "\n",
    "_ensure_pkgs()\n",
    "\n",
    "\n",
    "def _ensure_ffmpeg():\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except Exception:\n",
    "        raise RuntimeError(\n",
    "            \"ffmpeg is required.\\n\"\n",
    "            \"Conda: conda install -c conda-forge ffmpeg\\n\"\n",
    "            \"Ubuntu: sudo apt-get install -y ffmpeg\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _list_audio_files(root: Path) -> List[Path]:\n",
    "    return [p for p in root.rglob(\"*\") if p.suffix.lower() in AUDIO_EXTS and p.is_file()]\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def _export_slice(file_path: Path, start_s: float, end_s: float) -> Path:\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    piece = audio[int(start_s*1000):int(end_s*1000)]\n",
    "    tmp = AUDIO_OUT / f\"tmp_{file_path.stem}_{int(start_s)}_{int(end_s)}.wav\"\n",
    "    piece.export(tmp, format=\"wav\")\n",
    "    return tmp\n",
    "\n",
    "def _duration_seconds(file_path: Path) -> float:\n",
    "    a = AudioSegment.from_file(file_path)\n",
    "    return len(a) / 1000.0\n",
    "\n",
    "\n",
    "def _slice_fixed(file_path: Path, segment_seconds: int = FIXED_SEGMENT_SECONDS) -> List[Tuple[float, float]]:\n",
    "    dur_s = _duration_seconds(file_path)\n",
    "    if dur_s <= segment_seconds:\n",
    "        return [(0.0, dur_s)]\n",
    "    n = math.ceil(dur_s / segment_seconds)\n",
    "    return [(i*segment_seconds, min((i+1)*segment_seconds, dur_s)) for i in range(n)]\n",
    "\n",
    "def _slice_vad(file_path: Path) -> List[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Uses Silero VAD (torch.hub) to detect speech segments and returns a list of (start, end) in seconds.\n",
    "    Applies padding, merging, and length normalization for edge stability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        torch.set_num_threads(max(1, torch.get_num_threads()))  \n",
    "      \n",
    "        if \"_silero_model\" not in globals():\n",
    "            print(\"üß† Loading Silero VAD...\")\n",
    "            globals()[\"_silero_model\"], utils = torch.hub.load(\n",
    "                repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False, onnx=False\n",
    "            )\n",
    "            (globals()[\"_get_speech_timestamps\"],\n",
    "             globals()[\"_save_audio\"],\n",
    "             globals()[\"_read_audio\"],\n",
    "             globals()[\"_VAD_utils\"]) = utils\n",
    "        model = globals()[\"_silero_model\"]\n",
    "        get_speech_timestamps = globals()[\"_get_speech_timestamps\"]\n",
    "        read_audio = globals()[\"_read_audio\"]\n",
    "        VAD_utils = globals()[\"_VAD_utils\"]\n",
    "\n",
    "        wav = read_audio(str(file_path), sampling_rate=16000)  \n",
    "        sr = 16000\n",
    "\n",
    "     \n",
    "        print(\"üó£Ô∏è Running VAD‚Ä¶\")\n",
    "        speech_ts = get_speech_timestamps(\n",
    "            wav, model, sampling_rate=sr,\n",
    "            threshold=0.5,               \n",
    "            min_speech_duration_ms=int(VAD_MIN_SPEECH*1000),\n",
    "            min_silence_duration_ms=200,\n",
    "            window_size_samples=1536\n",
    "        )\n",
    "\n",
    "        if not speech_ts:\n",
    "            print(\"‚ÑπÔ∏è No speech detected; using fixed slicing fallback.\")\n",
    "            return _slice_fixed(file_path)\n",
    "\n",
    "      \n",
    "        segs = []\n",
    "        for ts in speech_ts:\n",
    "            s = max(0.0, ts[\"start\"] / sr - VAD_PAD)\n",
    "            e = ts[\"end\"] / sr + VAD_PAD\n",
    "            segs.append((s, e))\n",
    "\n",
    "        \n",
    "        merged = []\n",
    "        for s, e in sorted(segs):\n",
    "            if not merged:\n",
    "                merged.append([s, e])\n",
    "                continue\n",
    "            ps, pe = merged[-1]\n",
    "            if s - pe <= VAD_MERGE_GAP:\n",
    "                merged[-1][1] = max(pe, e)\n",
    "            else:\n",
    "                merged.append([s, e])\n",
    "\n",
    "    \n",
    "        normalized = []\n",
    "        for s, e in merged:\n",
    "            dur = e - s\n",
    "            if dur > VAD_MAX_SPEECH:\n",
    "               \n",
    "                k = math.ceil(dur / VAD_MAX_SPEECH)\n",
    "                step = dur / k\n",
    "                for i in range(k):\n",
    "                    ss = s + i*step\n",
    "                    ee = min(s + (i+1)*step, e)\n",
    "                    normalized.append((ss, ee))\n",
    "            elif dur < VAD_MIN_SPEECH:\n",
    "               \n",
    "                file_dur = _duration_seconds(file_path)\n",
    "                extra = (VAD_MIN_SPEECH - dur) / 2.0\n",
    "                ss = max(0.0, s - extra)\n",
    "                ee = min(file_dur, e + extra)\n",
    "                normalized.append((ss, ee))\n",
    "            else:\n",
    "                normalized.append((s, e))\n",
    "\n",
    "        \n",
    "        file_dur = _duration_seconds(file_path)\n",
    "        final = []\n",
    "        for s, e in normalized:\n",
    "            s = max(0.0, min(s, file_dur))\n",
    "            e = max(0.0, min(e, file_dur))\n",
    "            if e - s > 0.15:\n",
    "                final.append((s, e))\n",
    "\n",
    "        print(f\"‚úÖ VAD produced {len(final)} segment(s).\")\n",
    "        return final or _slice_fixed(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è VAD unavailable or failed ({e}); using fixed slicing.\")\n",
    "        return _slice_fixed(file_path)\n",
    "\n",
    "\n",
    "def transcribe_audio_file(file_path: Path) -> Dict[str, Any]:\n",
    "    from faster_whisper import WhisperModel\n",
    "    _ensure_ffmpeg()\n",
    "\n",
    "    print(f\"üéß Transcribing: {file_path.name}\")\n",
    "    model = WhisperModel(FW_MODEL_NAME, compute_type=FW_COMPUTE_TYPE)\n",
    "\n",
    "\n",
    "    slices = _slice_vad(file_path) if USE_VAD else _slice_fixed(file_path)\n",
    "\n",
    "    segments_out, full_text = [], []\n",
    "    for (s, e) in slices:\n",
    "        try:\n",
    "            tmpwav = _export_slice(file_path, s, e)\n",
    "            segs, info = model.transcribe(str(tmpwav), language=None, vad_filter=False, beam_size=1)\n",
    "            text_buf = []\n",
    "            for seg in segs:\n",
    "                rec = {\"file\": str(file_path), \"start\": float(s + seg.start), \"end\": float(s + seg.end), \"text\": seg.text.strip()}\n",
    "                segments_out.append(rec)\n",
    "                all_audio_segments.append(rec)\n",
    "                if seg.text.strip():\n",
    "                    text_buf.append(seg.text.strip())\n",
    "            full_text.append(\" \".join(text_buf))\n",
    "        except Exception as ex:\n",
    "            print(f\"   ‚ùå slice {s:.0f}-{e:.0f}s failed: {ex}\")\n",
    "\n",
    "    combined_text = \" \".join([t for t in full_text if t]).strip()\n",
    "    return {\"file\": str(file_path), \"segments\": segments_out, \"text\": combined_text}\n",
    "\n",
    "\n",
    "def _chunk_transcript(text: str, meta_file: str) -> List[Dict]:\n",
    "    if not text: return []\n",
    "    chunks, pending, refs = [], \"\", []\n",
    "\n",
    "    def _emit():\n",
    "        nonlocal pending, refs\n",
    "        if pending.strip():\n",
    "            chunks.append({\"type\":\"Chunk\",\"text\":pending.strip(),\n",
    "                           \"metadata\":{\"source_file\": meta_file,\n",
    "                                       \"orig_elements\":[{\"file\": meta_file,\"type\":\"AudioTranscript\"}]}})\n",
    "        pending, refs = \"\", []\n",
    "\n",
    "    if len(text) < CHUNK_SETTINGS[\"combine_text_under_n_chars\"]:\n",
    "        pending = text; refs=[{\"file\": meta_file,\"type\":\"AudioTranscript\"}]; _emit(); return chunks\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\"\\n\") if p.strip()] or [text]\n",
    "    for p in parts:\n",
    "        if len(pending) + len(p) + 2 <= CHUNK_SETTINGS[\"new_after_n_chars\"]:\n",
    "            pending += ((\"\\n\\n\"+p) if pending else p)\n",
    "        else:\n",
    "            _emit()\n",
    "            start = 0; maxc = CHUNK_SETTINGS[\"max_characters\"]\n",
    "            while start < len(p):\n",
    "                piece = p[start:start+maxc]\n",
    "                chunks.append({\"type\":\"Chunk\",\"text\":piece,\n",
    "                               \"metadata\":{\"source_file\": meta_file,\n",
    "                                           \"orig_elements\":[{\"file\": meta_file,\"type\":\"AudioTranscript\"}]}})\n",
    "                start += maxc\n",
    "    _emit()\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_all_audio() -> Dict[str, Any]:\n",
    "    audio_file_stats.clear(); all_audio_segments.clear(); all_audio_chunks.clear(); all_audio_transcripts.clear()\n",
    "\n",
    "    files = _list_audio_files(AUDIO_ROOT)\n",
    "    print(f\"üìÇ Found {len(files)} audio file(s) under {AUDIO_ROOT}\")\n",
    "    if not files: return {\"files\": 0}\n",
    "\n",
    "    transcripts_path = AUDIO_OUT / \"transcripts.jsonl\"\n",
    "    with open(transcripts_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "        for f in files:\n",
    "            res = transcribe_audio_file(f)\n",
    "            all_audio_transcripts.append(res)\n",
    "            jf.write(json.dumps({\"type\":\"transcript\", **res}, ensure_ascii=False) + \"\\n\")\n",
    "            file_chunks = _chunk_transcript(res[\"text\"], res[\"file\"])\n",
    "            all_audio_chunks.extend(file_chunks)\n",
    "            audio_file_stats[f.stem] = {\"segments\": len(res[\"segments\"]), \"has_text\": bool(res[\"text\"]), \"chunks\": len(file_chunks)}\n",
    "            print(f\"   ‚úÖ {f.name}: segments={len(res['segments'])}, chunks={len(file_chunks)}\")\n",
    "\n",
    "    print(f\"üíæ Transcripts saved: {transcripts_path}\")\n",
    "    print(f\"üìä Total chunks built: {len(all_audio_chunks)}\")\n",
    "    return {\"files\": len(files), \"chunks\": len(all_audio_chunks)}\n",
    "\n",
    "\n",
    "def summarize_audio_chunks(max_chunks: Optional[int] = None, max_chars_per_chunk: int = 4000) -> List[Dict[str, Any]]:\n",
    "    if 'summarize_chain' not in globals():\n",
    "        raise RuntimeError(\"summarize_chain not found. Run your Gemma setup.\")\n",
    "\n",
    "    n = len(all_audio_chunks) if max_chunks is None else min(max_chunks, len(all_audio_chunks))\n",
    "    print(f\"üìù Summarizing {n} audio chunk(s)‚Ä¶\")\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for i in range(n):\n",
    "        ch = all_audio_chunks[i]; txt = (ch.get(\"text\") or \"\").strip()\n",
    "        if not txt: continue\n",
    "        excerpt = (txt[:max_chars_per_chunk]+\"... [truncated]\") if len(txt) > max_chars_per_chunk else txt\n",
    "        try:\n",
    "            summary = summarize_chain.invoke(excerpt)\n",
    "            out.append({\"chunk_index\": i, \"source_file\": ch.get(\"metadata\",{}).get(\"source_file\"),\n",
    "                        \"summary\": summary, \"excerpt\": excerpt[:300]})\n",
    "            if (i+1) % 25 == 0 or i == n-1: print(f\"   ‚úÖ {i+1}/{n}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå chunk {i+1}/{n} failed: {e}\")\n",
    "    print(f\"‚úÖ Audio summarization done. {len(out)} summaries.\")\n",
    "    return out\n",
    "\n",
    "def audio_summarization_pipeline(save_jsonl: bool = True, out_path: Optional[Path] = None) -> Dict[str, Any]:\n",
    "    stats = process_all_audio()\n",
    "    text_sums = summarize_audio_chunks()\n",
    "    results = {\"stats\": stats, \"text_summaries\": text_sums}\n",
    "    if save_jsonl:\n",
    "        target = out_path or (AUDIO_OUT / \"audio_summaries.jsonl\")\n",
    "        with open(target, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in text_sums:\n",
    "                f.write(json.dumps({\"type\":\"audio_text_summary\", **r}, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"üíæ Saved audio summaries to: {target}\")\n",
    "    print(\"üéâ Audio pipeline finished.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_audio_summary_samples(text_summaries: List[Dict[str, Any]], n: int = 5):\n",
    "    print(\"\\nüîç SAMPLE AUDIO SUMMARIES\")\n",
    "    print(\"=\" * 28)\n",
    "    for i, s in enumerate(text_summaries[:n], 1):\n",
    "        print(f\"\\n#{i} file: {s.get('source_file')}, chunk: {s.get('chunk_index')}\")\n",
    "        print(f\"Excerpt: {s.get('excerpt','')[:160]}...\")\n",
    "        print(f\"Summary: {s.get('summary')}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
